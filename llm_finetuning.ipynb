{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jkk8jWTHayS"
      },
      "source": [
        "## Local LLMs fine-tuning with different quantization techniques (`bitsandbytes` and `gptq`)\n",
        "\n",
        "This notebooks provide a quick overview of using various quantization techniques to fine-tune LLMs on comodity hardware (memory constrained). Especially on Colab GPU (free-tier), to fine-tune small LLM variant (7B) with 16GiB, quantization techniques like 4-bit quantization and GPTQ is needed to prevent Out-of-Memory errors with long sequences length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLVZdYHNIRd5"
      },
      "source": [
        "Install prerequisite packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAkBhRxYhJ1R"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/taprosoft/llm_finetuning/\n",
        "%cd llm_finetuning\n",
        "!pip install -r requirements.txt\n",
        "!pip install -r cuda_quant_requirements.txt\n",
        "!wandb disabled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsXtmFArI3-9"
      },
      "source": [
        "Download some model weights from HuggingFace [model hub](https://huggingface.co/models) using the `download_model.py` script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2NsuwcFrZ4F"
      },
      "outputs": [],
      "source": [
        "!mkdir models\n",
        "# download a 7B GPTQ base model\n",
        "!python download_model.py TheBloke/open-llama-7b-open-instruct-GPTQ\n",
        "# download a normal 7B model (note that we have to use sharded checkpoint due to memory limit of Colab)\n",
        "!python download_model.py CleverShovel/vicuna-7b-v1.3-sharded-bf16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leNY_lU8Ji-m"
      },
      "source": [
        "Use `finetune.py` script to run training / inference. We first perform evaluation of the downloaded models on a public instruction-tuning datasets.\n",
        "\n",
        "To understand the format of the dataset, take a look at [alpaca-cleaned](https://huggingface.co/datasets/yahma/alpaca-cleaned) or the guideline in [README](https://github.com/taprosoft/llm_finetuning).\n",
        "\n",
        "It looks something likes this:\n",
        "\n",
        "```json\n",
        "[\n",
        "    {\n",
        "        \"instruction\": \"do something with the input\",\n",
        "        \"input\": \"input string\",\n",
        "        \"output\": \"output string\"\n",
        "    }\n",
        "]\n",
        "```\n",
        "\n",
        "We start with the 7B model on 4-bit quantization mode from `bitsandbytes`. Take a look at the output loss and processing time per step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFSrStbSASst"
      },
      "outputs": [],
      "source": [
        "!python finetune.py \\\n",
        "    --base_model 'models/CleverShovel_vicuna-7b-v1.3-sharded-bf16' \\\n",
        "    --data_path 'yahma/alpaca-cleaned' \\\n",
        "    --output_dir 'output_lora' \\\n",
        "    --batch_size 32 \\\n",
        "    --micro_batch_size 1 \\\n",
        "    --train_on_inputs True \\\n",
        "    --num_epochs 1 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --cutoff_len 1600 \\\n",
        "    --group_by_length \\\n",
        "    --val_set_size 0.05 \\\n",
        "    --eval_steps 0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 5 \\\n",
        "    --gradient_checkpointing 1 \\\n",
        "    --mode 4 \\\n",
        "    --eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU61FgkgLqze"
      },
      "source": [
        "Now we will run the same script with GPTQ quantization mode (`--mode gptq`). Note that we need to switch to a compatible model weight to be used with this method. (look for `gptq` in the model name). We can see some significant difference in processing time using different quantization methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdy5ey7QnS0L"
      },
      "outputs": [],
      "source": [
        "# a hotfix for Colab compatibility issue of peft\n",
        "!pip install peft==0.3.0\n",
        "!python finetune.py \\\n",
        "    --base_model 'models/TheBloke_open-llama-7b-open-instruct-GPTQ' \\\n",
        "    --data_path 'yahma/alpaca-cleaned' \\\n",
        "    --output_dir 'output_lora' \\\n",
        "    --batch_size 32 \\\n",
        "    --micro_batch_size 1 \\\n",
        "    --train_on_inputs True \\\n",
        "    --num_epochs 1 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --cutoff_len 1600 \\\n",
        "    --group_by_length \\\n",
        "    --val_set_size 0.05 \\\n",
        "    --eval_steps 0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 5 \\\n",
        "    --gradient_checkpointing 1 \\\n",
        "    --mode gptq \\\n",
        "    --eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peTZ8WmF0hDy"
      },
      "source": [
        "Evaluation loop only provides the loss and run time measurement. To actually see the model output in text format, use `inference.py` script. Note that perform inference / generation will take much longer time than evaluation loop due to the additional overhead in token generation steps. We will use `exllama` inference backend to speed up the inference time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoU8HrXT0hDy"
      },
      "outputs": [],
      "source": [
        "# to fix some Colab install issue with Exllama\n",
        "!git clone https://github.com/taprosoft/exllama.git\n",
        "!cd exllama && pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWpgxljC0hDz"
      },
      "outputs": [],
      "source": [
        "!python inference.py \\\n",
        "    --base models/TheBloke_open-llama-7b-open-instruct-GPTQ \\\n",
        "    --mode exllama \\\n",
        "    --data 'yahma/alpaca-cleaned' \\\n",
        "    --selected_ids [0,1,2,3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSgvc9ZxL6wy"
      },
      "source": [
        "Now we can start training. On a relatively old GPU like T4, it can take about 20-30h to complete the training on Alpaca dataset. Output checkpoint is stored in `output_lora`. Checkpoint is created at regular interval so you can stop earlier if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6J17VYR1MHav"
      },
      "outputs": [],
      "source": [
        "!python finetune.py \\\n",
        "    --base_model 'models/TheBloke_open-llama-7b-open-instruct-GPTQ' \\\n",
        "    --data_path 'yahma/alpaca-cleaned' \\\n",
        "    --output_dir 'output_lora' \\\n",
        "    --batch_size 32 \\\n",
        "    --micro_batch_size 1 \\\n",
        "    --train_on_inputs True \\\n",
        "    --num_epochs 1 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --cutoff_len 1600 \\\n",
        "    --group_by_length \\\n",
        "    --val_set_size 0.05 \\\n",
        "    --eval_steps 0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 5 \\\n",
        "    --gradient_checkpointing 1 \\\n",
        "    --mode gptq"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
